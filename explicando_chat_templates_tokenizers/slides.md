---
theme: apple-basic
background: https://raw.githubusercontent.com/huggingface/smol-course/refs/heads/main/banner.png
transition: slide-left
title: Chat Templates & Tokenizers
mdc: true
layout: intro-image
image: 'https://raw.githubusercontent.com/huggingface/smol-course/refs/heads/main/banner.png'
---

<div class="absolute bottom-10">
  <h1>Explicando</h1>
  <p>Chat Templates & Tokenizers</p>
</div>


<!--
Bem-vindos! Hoje vamos explorar chat templates e tokenizers, conceitos fundamentais para trabalhar com LLMs.
Vamos ver como funcionam os modelos de instru√ß√£o e como estruturar conversas de forma eficaz.
-->

---
layout: default
---

# O que vamos ver hoje?

<Toc maxDepth="1"></Toc>

<!--
Preparei uma apresenta√ß√£o bem pr√°tica e objetiva. Vamos cobrir desde os conceitos b√°sicos at√© exemplos avan√ßados.
Todo o c√≥digo estar√° dispon√≠vel para voc√™s testarem depois.
-->

---
transition: slide-up
---

# Modelos Base vs Modelos de Instru√ß√£o

Entendendo a diferen√ßa fundamental

- **Modelo Base** (`SmolLM3-3B-Base`)
  - Treinado em texto bruto
  - Prev√™ o pr√≥ximo token
  - Completa sequ√™ncias de texto

- **Modelo de Instru√ß√£o** (`SmolLM3-3B`)
  - Fine-tuned para seguir instru√ß√µes
  - Entende perguntas e comandos
  - Responde como assistente

<!--
A diferen√ßa √© crucial: modelos base apenas continuam texto, enquanto modelos de instru√ß√£o entendem contexto de conversa.
Os chat templates s√£o a ponte entre esses dois mundos - eles ensinam o modelo a estruturar conversas.
-->

---
layout: two-cols
---

# A Transforma√ß√£o

Do texto bruto para conversas estruturadas

::left::

**Modelo Base**
```
Input: "O tempo hoje est√°"
Output: "ensolarado e quente"
```

Apenas continua o texto

::right::

**Modelo de Instru√ß√£o**
```
Input: "Como est√° o tempo?"
Output: "Posso verificar para voc√™..."
```

Entende e responde

<!--
Vejam a diferen√ßa: o modelo base apenas completa frases. O modelo de instru√ß√£o entende que √© uma pergunta e responde adequadamente.
Essa transforma√ß√£o acontece atrav√©s de supervised fine-tuning usando chat templates.
-->

---

# O que s√£o Chat Templates?

Formato estruturado para conversas

- **Defini√ß√£o**: "Gram√°tica" para estruturar intera√ß√µes
- **Fun√ß√£o**: Ensina modelos a distinguir pap√©is
- **Padr√£o**: SmolLM3 usa ChatML (Chat Markup Language)
- **Benef√≠cio**: Consist√™ncia e clareza

```xml
<|im_start|>system
Voc√™ √© um assistente prestativo.<|im_end|>
<|im_start|>user
Ol√°!<|im_end|>
<|im_start|>assistant
Como posso ajudar?<|im_end|>
```

<!--
Chat templates s√£o como regras gramaticais para conversas com IA. 
Eles definem como estruturar mensagens, separar diferentes pap√©is (sistema, usu√°rio, assistente).
O formato ChatML usa tokens especiais que marcam in√≠cio e fim de cada mensagem.
-->

---
layout: two-cols
---

# Usando Pipeline - A forma mais f√°cil

Abstra√ß√£o que gerencia tudo automaticamente

<<< @/code/01_pipeline_basico.py

::right::

<div class="mt-12">

**O Pipeline faz:**
- Aplica chat template
- Tokeniza mensagens
- Gerencia gera√ß√£o
- Retorna output estruturado

</div>

<!--
O pipeline √© a forma mais simples de usar LLMs. Ele esconde toda a complexidade dos chat templates.
Voc√™ s√≥ precisa passar uma lista de mensagens e ele cuida do resto.
Perfeito para come√ßar ou para uso em produ√ß√£o.
-->

---

# Estrutura de Mensagens

Anatomia de uma conversa

<<< @/code/02_estrutura_mensagens.py

**Tipos de Mensagem:**
- `system`: Define comportamento do modelo
- `user`: Perguntas e comandos do usu√°rio
- `assistant`: Respostas da IA
- `tool`: Resultados de fun√ß√µes (avan√ßado)

<!--
Cada mensagem tem dois campos essenciais: role e content.
O role identifica quem est√° falando - sistema, usu√°rio ou assistente.
Essa estrutura simples mas poderosa permite conversas complexas e multiturno.
-->

---

# Aplicando Chat Templates Manualmente

Controle direto sobre a formata√ß√£o

<<< @/code/03_aplicar_template.py

**Par√¢metros importantes:**
- `tokenize=False`: Retorna string em vez de tokens
- `add_generation_prompt=True`: Adiciona prompt para resposta

<!--
Quando voc√™ precisa de mais controle, pode aplicar o chat template manualmente.
O tokenizer j√° vem configurado com o template correto para cada modelo.
Esse √© o m√©todo usado internamente pelo pipeline.
-->

---
layout: two-cols
---

# Generation Prompt

Controlando quando o modelo responde

<<< @/code/04_generation_prompt.py {all|14-18|21-25}

::right::

<div class="mt-12">

**Quando usar:**

‚úÖ `True` ‚Üí Infer√™ncia  
‚ùå `False` ‚Üí Treinamento

</div>

<!--
O generation prompt √© crucial! Ele diz ao modelo "agora √© sua vez de falar".
Sem ele, o modelo pode continuar a mensagem do usu√°rio em vez de responder.
Para treinamento usamos False porque j√° temos as respostas completas.
-->

---

# Modo Thinking do SmolLM3

Racioc√≠nio vis√≠vel vs invis√≠vel

<<< @/code/05_thinking_mode.py

**Benef√≠cios do Thinking Mode:**
- Mostra processo de racioc√≠nio
- √ötil para tarefas complexas
- Transpar√™ncia nas respostas
- Debugging mais f√°cil

<!--
Uma feature especial do SmolLM3 √© o modo thinking.
Ele pode mostrar ou esconder o racioc√≠nio interno.
Perfeito para matem√°tica, l√≥gica, ou quando voc√™ quer entender como o modelo chegou √† resposta.
-->

---

# Conversas Multi-turno

Mantendo contexto atrav√©s de m√∫ltiplas intera√ß√µes

<<< @/code/06_conversa_multiturno.py

<!--
Conversas multi-turno s√£o fundamentais para assistentes √∫teis.
O modelo mant√©m contexto de mensagens anteriores.
Voc√™ simplesmente adiciona novas mensagens √† lista e passa de volta para o pipeline.
-->

---

# System Messages

Definindo o comportamento do modelo

<<< @/code/07_system_messages.py

**Dicas para System Messages:**
- Seja espec√≠fico e claro
- Defina limites e expectativas
- Use exemplos quando poss√≠vel
- √â a primeira mensagem da conversa

<!--
System messages s√£o super poderosos - eles definem toda a personalidade do modelo.
Podem transformar o mesmo modelo em um assistente t√©cnico, criativo, ou profissional.
Invista tempo criando bons system prompts - faz toda a diferen√ßa!
-->

---

# Formato ChatML - Estrutura

Tokens especiais que delimitam conversas

```xml
<|im_start|>system
Voc√™ √© um assistente t√©cnico focado em programa√ß√£o.<|im_end|>
<|im_start|>user
Ol√°!<|im_end|>
<|im_start|>assistant
Como posso ajudar?<|im_end|>
<|im_start|>user
Explique fun√ß√µes em Python.<|im_end|>
<|im_start|>assistant
```

**Componentes:**
- `<|im_start|>` e `<|im_end|>`: Delimitadores
- Roles: system, user, assistant, tool
- Content: Conte√∫do entre os delimitadores

<!--
O ChatML usa tokens especiais bem definidos.
Esses tokens ensinam o modelo onde cada mensagem come√ßa e termina.
√â importante n√£o adicionar esses tokens manualmente - deixe o template fazer isso!
-->

---
layout: two-cols
---

# Continue Final Message

Controlando continua√ß√£o de respostas

::left::

**Caso de uso: JSON**
```python
messages = [
    {"role": "user", 
     "content": "Responda em JSON"},
    {"role": "assistant", 
     "content": '{"nome": "'}
]

# continue_final_message=True
# Modelo completa: Jo√£o",
```

::right::

**Aplica√ß√µes:**
- Output estruturado
- Completar c√≥digo
- Guiar racioc√≠nio
- For√ßar formatos

<!--
Continue final message √© uma t√©cnica avan√ßada mas muito √∫til.
Voc√™ "preenche parcialmente" a resposta do assistente e o modelo completa.
√ìtimo para garantir formato JSON, completar c√≥digo, ou guiar o racioc√≠nio passo a passo.
-->

---

# Par√¢metros de Gera√ß√£o

Controlando a criatividade do modelo

```python
generation_config = {
    "max_new_tokens": 200,      # M√°ximo de tokens a gerar
    "temperature": 0.8,          # Criatividade (0-2)
    "do_sample": True,           # Usar sampling
    "top_p": 0.9,               # Nucleus sampling
    "repetition_penalty": 1.1    # Evitar repeti√ß√£o
}
```

**Temperature:**
- 0.1-0.3: Respostas focadas e determin√≠sticas
- 0.7-0.9: Balanceado (recomendado)
- 1.5-2.0: Muito criativo/aleat√≥rio

<!--
Os par√¢metros de gera√ß√£o s√£o como voc√™ afina o comportamento do modelo.
Temperature √© o mais importante - baixa para tarefas factuais, alta para criatividade.
Top_p e repetition_penalty ajudam a manter respostas naturais e variadas.
-->

---

# Function Calling / Tool Usage

Permitindo o modelo usar ferramentas externas

```python
tools = [{
    "type": "function",
    "function": {
        "name": "get_weather",
        "description": "Obt√©m clima atual de uma localiza√ß√£o",
        "parameters": {
            "type": "object",
            "properties": {
                "location": {"type": "string"},
                "unit": {"type": "string", "enum": ["celsius", "fahrenheit"]}
            },
            "required": ["location"]
        }
    }
}]
```

<!--
Function calling permite que LLMs interajam com o mundo externo.
O modelo aprende quando e como chamar fun√ß√µes baseado nas descri√ß√µes.
Voc√™ define as ferramentas dispon√≠veis e o modelo decide quando us√°-las.
-->

---

# Boas Pr√°ticas - Template Consistency

Regras de ouro para usar chat templates

‚úÖ **FA√áA:**
- Use o mesmo template para treino e infer√™ncia
- Use `add_generation_prompt=True` para infer√™ncia
- Use `add_generation_prompt=False` para treino
- Deixe o tokenizer gerenciar tokens especiais
- Teste templates antes de treinar

‚ùå **N√ÉO FA√áA:**
- Adicionar tokens especiais manualmente
- Misturar diferentes formatos de template
- Ignorar system messages importantes
- Esquecer de validar estrutura de mensagens

<!--
Consist√™ncia √© crucial! Usar templates diferentes entre treino e infer√™ncia quebra o modelo.
Nunca adicione tokens especiais manualmente - o template j√° faz isso.
Essas pr√°ticas podem parecer simples, mas evitam 90% dos problemas comuns.
-->

---

# Debugging de Templates

Inspecionando o que est√° acontecendo

```python
# Ver o template do modelo
print(tokenizer.chat_template)

# Ver tokens especiais
print(f"BOS: {tokenizer.bos_token}")
print(f"EOS: {tokenizer.eos_token}")

# Aplicar template e ver resultado
formatted = tokenizer.apply_chat_template(
    messages, 
    tokenize=False
)
print(repr(formatted))  # Mostra caracteres de escape
```

<!--
Quando algo n√£o funciona, √© hora de debugar.
Inspecionar o template ajuda a entender o que est√° acontecendo.
O repr() mostra todos os caracteres especiais, muito √∫til para debugging.
-->

---

# Casos de Uso Reais

Aplica√ß√µes pr√°ticas de chat templates

1. **Chatbots de Atendimento**
   - System message define tom profissional
   - Multi-turno para contexto

2. **Assistentes de C√≥digo**
   - System message com expertise t√©cnica
   - Continue final message para completar c√≥digo

3. **Tutores Educacionais**
   - Thinking mode para mostrar racioc√≠nio
   - System message pedag√≥gico

4. **Agentes com Ferramentas**
   - Function calling para APIs
   - Tool messages para feedback

<!--
Templates n√£o s√£o s√≥ teoria - t√™m aplica√ß√µes pr√°ticas importantes.
Cada caso de uso se beneficia de diferentes aspectos dos templates.
A chave √© escolher a combina√ß√£o certa de features para seu problema.
-->

---

# Armadilhas Comuns

Erros frequentes e como evit√°-los

‚ùå **Template Mismatch**
- Usar template diferente do que o modelo foi treinado

‚ùå **Tokens Duplicados**
- Adicionar tokens quando template j√° inclui

‚ùå **System Message Ausente**
- N√£o fornecer contexto suficiente

‚ùå **Generation Prompt Errado**
- Usar `True` quando deveria ser `False` (ou vice-versa)

‚ùå **Context Overflow**
- N√£o gerenciar tamanho de conversas longas

<!--
Aprenda com os erros dos outros! Essas s√£o as armadilhas mais comuns.
Template mismatch √© provavelmente o erro n√∫mero 1 - sempre confira!
Context overflow acontece em conversas muito longas - implemente truncamento.
-->

---
layout: two-cols
---

# Performance e Otimiza√ß√£o

Dicas para produ√ß√£o

::left::

**Otimiza√ß√µes:**
- Cache templates formatados
- Batch processing
- Truncamento inteligente
- Monitore token usage

::right::

**M√©tricas importantes:**
- Taxa de erro em templates
- Lat√™ncia m√©dia
- Token usage
- Qualidade de respostas

<!--
Em produ√ß√£o, performance importa muito.
Cachear templates formatados economiza processamento.
Monitorar m√©tricas ajuda a identificar problemas cedo.
-->

---

# Pr√≥ximos Passos

Onde ir a partir daqui

üìö **Aprender:**
- Supervised Fine-Tuning (pr√≥ximo t√≥pico)
- Preference Alignment (RLHF/DPO)
- Custom template creation
- Multimodal templates

üîß **Praticar:**
- Criar seu pr√≥prio chatbot
- Fine-tune SmolLM3 para seu dom√≠nio
- Experimentar com thinking mode
- Implementar function calling

<!--
Chat templates s√£o a base - agora voc√™ est√° pronto para o pr√≥ximo n√≠vel.
Supervised fine-tuning usa tudo que vimos hoje para treinar modelos.
A melhor forma de aprender √© praticando - escolha um projeto e m√£os √† obra!
-->

---

# Recursos e Documenta√ß√£o

Links √∫teis para aprofundar

**Documenta√ß√£o Oficial:**
- [Chat Templates Basics](https://huggingface.co/docs/transformers/chat_template_basics)
- [SmolLM3 Model Card](https://huggingface.co/HuggingFaceTB/SmolLM3-3B)
- [TRL Documentation](https://huggingface.co/docs/trl)

**Comunidade:**
- [Hugging Face Forum](https://discuss.huggingface.co/)
- [Discord](https://discord.gg/UrrTSsSyjb)

**Dataset:**
- [SmolTalk2](https://huggingface.co/datasets/HuggingFaceTB/smoltalk2)

<!--
Deixo aqui recursos excelentes para voc√™s continuarem estudando.
A documenta√ß√£o do Hugging Face √© muito completa e bem escrita.
A comunidade √© super ativa - n√£o tenham medo de fazer perguntas!
-->

---
layout: center
class: text-center
---

# Obrigado! üéâ

Perguntas?

<div class="pt-12">
  <span class="text-sm opacity-75">
    Baseado no conte√∫do: Hugging Face Smol Course - Unit 1.2
  </span>
</div>

<!--
E √© isso! Espero que tenham gostado e aprendido bastante.
Chat templates podem parecer simples, mas s√£o fundamentais para trabalhar bem com LLMs.
Fiquem √† vontade para perguntar qualquer coisa!
-->
