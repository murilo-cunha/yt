---
theme: apple-basic
background: https://raw.githubusercontent.com/huggingface/smol-course/refs/heads/main/banner.png
title: Chat Templates & Tokenizers
mdc: true
layout: intro-image
image: 'https://raw.githubusercontent.com/huggingface/smol-course/refs/heads/main/banner.png'
---

<div class="absolute bottom-10">
  <h1>Explicando</h1>
  <p>Chat Templates & Tokenizers</p>
</div>


<!--
Bem-vindos! Hoje vamos explorar chat templates e tokenizers, conceitos fundamentais para trabalhar com LLMs.
Vamos ver como funcionam os modelos de instru√ß√£o e como estruturar conversas de forma eficaz.
-->

---
layout: two-cols
---

# Modelos Base vs Modelos de Instru√ß√£o

Entendendo a diferen√ßa fundamental

<v-clicks>

- **Modelo Base** (`SmolLM3-3B-Base`)
  - Treinado em texto bruto
  - Prev√™ o pr√≥ximo token
  - Completa sequ√™ncias de texto

<div class="mt-8"></div>

- **Modelo de Instru√ß√£o** (`SmolLM3-3B`)
  - Fine-tuned para seguir instru√ß√µes
  - Entende perguntas e comandos
  - Responde como assistente

</v-clicks>

::right::


<div class="flex flex-col gap-4">
<v-click>
  <div>
    <p class="text-sm mb-2 font-semibold">Next Token Prediction:</p>
    <img src="https://jalammar.github.io/images/gpt3/05-gpt3-generate-output-context-window.gif" alt="Next Token Prediction" class="rounded w-full">
  </div>
</v-click>

<v-click>
  
  <div>
    <p class="text-sm mb-2 font-semibold">Conversation Structure:</p>
    
```json
[
  {"role": "user", 
   "content": "Como est√° o tempo?"},
  {"role": "assistant", 
   "content": "Posso verificar..."}
]
```
  </div>
</v-click>
</div>

<!--
A diferen√ßa √© crucial: modelos base apenas continuam texto, enquanto modelos de instru√ß√£o entendem contexto de conversa.
Os chat templates s√£o a ponte entre esses dois mundos - eles ensinam o modelo a estruturar conversas.
-->

---

# Pap√©is em Chat Templates

Entendendo system, user e assistant

- **System**: Define comportamento e personalidade do modelo
- **User**: Mensagens do usu√°rio/cliente
- **Assistant**: Respostas geradas pelo modelo

<<< @/code/07_system_messages.py#code {*}{maxHeight:'250px'}

<!--
Cada papel tem sua fun√ß√£o espec√≠fica:
- System message √© como dar instru√ß√µes permanentes ao modelo
- User √© sempre quem est√° fazendo perguntas
- Assistant √© o modelo respondendo
√â importante manter essa estrutura para o modelo entender o contexto corretamente.
# Mensagens de sistema definem o comportamento do modelo
# S√£o a primeira mensagem e influenciam toda a conversa
-->

---

# Usando Pipeline - A forma mais f√°cil

Abstra√ß√£o que gerencia tudo automaticamente

<<< @/code/01_pipeline_basico.py#code

<!--**O Pipeline faz:**
- Aplica chat template
- Tokeniza mensagens
- Gerencia gera√ß√£o
- Retorna output estruturado

O pipeline √© a forma mais simples de usar LLMs. Ele esconde toda a complexidade dos chat templates.
Voc√™ s√≥ precisa passar uma lista de mensagens e ele cuida do resto.
Perfeito para come√ßar ou para uso em produ√ß√£o.
-->

---

# Aplicando Templates Sem Chamar o LLM

√ötil para prepara√ß√£o de dados e treinamento

O pipeline automaticamente transforma mensagens JSON em texto formatado usando chat templates por tr√°s dos planos.

<<< @/code/03_aplicar_template.py#code {*}{maxHeight:'320px'}

<!--
Quando preparamos dados para treinamento, n√£o queremos o generation prompt.
Isso permite que o modelo aprenda a gerar a resposta do assistente a partir do template completo.
√â especialmente √∫til para criar datasets de fine-tuning.
False = preparar dados | True = gerar respostas
-->

---

# Convertendo Conversas

## Formato JSON
```json {*|2|3|6|8|*}
[
    {"role": "system", "content": "Voc√™ √© um assistente t√©cnico."},
    {"role": "user", "content": "Explique o que √© um chat template?"},
    {
        "role": "assistant",
        "content": "Eles padronizam a convers√£o de di√°logos em um formato de texto que LLMs consiguem entender e processar corretamente.",
    },
    {"role": "user", "content": "Como √© esse formato?"},
]
```

## Formato ChatML

```xml {*|10|13|15|17|18}{at:1, lines:true, maxHeight:'190px'}
<|im_start|>system
## Metadata

Knowledge Cutoff Date: June 2025
Today Date: 02 January 2026
Reasoning Mode: /think

## Custom Instructions

Voc√™ √© um assistente t√©cnico.

<|im_start|>user
Explique o que √© um chat template?<|im_end|>
<|im_start|>assistant
Eles padronizam a convers√£o de di√°logos em um formato de texto que LLMs consiguem entender e processar corretamente.<|im_end|>
<|im_start|>user
Como √© esse formato?<|im_end|>
<|im_start|>assistant
```

---

# Generation Prompt

Controlando in√≠cio de resposta do modelo

O par√¢metro `add_generation_prompt` adiciona tokens que indicam que √© a vez do assistente responder.

<<< @/code/04_generation_prompt.py#code {*|13,18}{maxHeight:'340px'}

<!--
O generation prompt √© crucial para indicar ao modelo que deve come√ßar a responder.
Sem ele, o modelo pode n√£o entender que √© sua vez de falar.
Use True para infer√™ncia (gerar respostas) e False para preparar dados de treino.
-->

---

# Continue Final Message

Controle avan√ßado de resposta

O par√¢metro `continue_final_message=True` faz o modelo **continuar** a √∫ltima mensagem ao inv√©s de iniciar uma nova.

<<< @/code/08_continue_final_message.py#code {*|8|13,18}{maxHeight:'315px'}

<!--
Continue final message √© uma t√©cnica avan√ßada de "prefilling" - voc√™ come√ßa a resposta do assistente.
Muito √∫til para for√ßar formato espec√≠fico de sa√≠da, como JSON ou c√≥digo.
O modelo vai continuar exatamente de onde voc√™ parou, mantendo o formato.
N√£o pode usar add_generation_prompt=True junto com continue_final_message=True.
-->

---

# Aplica√ß√µes Pr√°ticas - Continue Final Message

Casos de uso reais

<v-clicks>

<div>

**1. Sa√≠da Estruturada (JSON)**
```python
{"role": "assistant", "content": '{"answer": "'}
```

<carbon-arrow-right/> Modelo completa: `Paris", "confidence": 0.95}`

</div>
<div>

**2. Completar C√≥digo**
```python
{"role": "assistant", "content": "def factorial(n):\n    return n * "}
```

Modelo completa: `factorial(n-1) if n > 1 else 1`

</div>
<div>

**3. Racioc√≠nio Guiado**
```python
{"role": "assistant", "content": "Vou resolver passo a passo:\n\nPasso 1: "}
```

<carbon-arrow-right/>  Modelo segue a estrutura sugerida

</div>
</v-clicks>

<!--
Continue final message √© poderoso para guiar o formato de sa√≠da.
Use para JSON, c√≥digo, ou qualquer formato estruturado.
Tamb√©m √∫til para guiar o modelo atrav√©s de racioc√≠nio passo a passo.
Lembre: a √∫ltima mensagem deve ter role "assistant".
-->

---

# Recap: Par√¢metros de `apply_chat_template`


```python
json_messages = [
    {"role": "user", "content": "Formate a resposta em JSON"},
    {"role": "assistant", "content": '{"name": "'},
]
```

<div class="grid grid-cols-3 gap-4">
<v-click>
  <div>
    <h3 class="text-lg font-semibold mb-2">add_generation_prompt=false</h3>
    
```python {4,11}
>>> tokenizer.apply_chat_template(
    json_messages,
    tokenize=False,
    add_generation_prompt=False,
)
"""
...
<|im_start|>user
Formate a resposta em JSON<|im_end|>
<|im_start|>assistant
{"name": "<|im_end|>
"""
```
  </div>
</v-click>
<v-click>
  <div>
    <h3 class="text-lg font-semibold mb-2">add_generation_prompt=true</h3>
    
```python {4,11-12}
>>> tokenizer.apply_chat_template(
    json_messages,
    tokenize=False,
    add_generation_prompt=True,
)
"""
...
<|im_start|>user
Formate a resposta em JSON<|im_end|>
<|im_start|>assistant
{"name": "<|im_end|>
<|im_start|>assistant
"""
```
  </div>
</v-click>
<v-click>
  <div>
    <h3 class="text-lg font-semibold mb-2">continue_final_message=true</h3>

```python {4,11}
>>> tokenizer.apply_chat_template(
    json_messages,
    tokenize=False,
    continue_final_message=True,
)
"""
...
<|im_start|>user
Formate a resposta em JSON<|im_end|>
<|im_start|>assistant
{"name": "
"""
```

  </div>
</v-click>
</div>

---

# Thinking Mode

Expondo racioc√≠nio do modelo

Alguns modelos suportam mostrar seu processo de pensamento antes da resposta final.

<<< @/code/05_thinking_mode.py#code {*|12}

<!--
Thinking mode √© uma feature avan√ßada que permite ver como o modelo est√° raciocinando.
√ötil para debugging, educa√ß√£o e para construir confian√ßa nas respostas.
Nem todos os modelos suportam - verifique a documenta√ß√£o do modelo.
O modelo gera primeiro seu racioc√≠nio interno, depois a resposta para o usu√°rio.
-->

---

# Function Calling / Tool Usage

Integrando LLMs com ferramentas externas

Chat templates suportam **function calling** - permitindo que modelos chamem APIs e ferramentas.

<<< @/code/10_tool_definition.py#code {*}{maxHeight:'340px'}

<!--
Function calling permite que LLMs interajam com o mundo real.
Ferramentas s√£o definidas usando JSON Schema - especificando nome, descri√ß√£o e par√¢metros.
O modelo aprende quando e como chamar cada ferramenta baseado na descri√ß√£o.
Par√¢metros obrigat√≥rios garantem que o modelo forne√ßa todas as informa√ß√µes necess√°rias.
-->

---

# Como Function Calling Afeta o Template

O template injeta defini√ß√µes de ferramentas no system message

<<< @/code/09_function_calling.py#code {*|6|7|9-10|11-20|22-26|27-30}{maxHeight:'400px'}

<!--
Quando voc√™ passa tools para apply_chat_template, acontece m√°gica!
O template automaticamente injeta as defini√ß√µes no system message em formato XML.
O modelo aprende a gerar tool_calls quando precisa de informa√ß√µes externas.
Voc√™ executa a ferramenta e retorna o resultado com role "tool".
O modelo ent√£o gera a resposta final usando o resultado da ferramenta.
-->

---

# Customiza√ß√£o Avan√ßada de Templates

Para casos de uso especializados

Voc√™ pode criar **templates customizados** usando sintaxe Jinja2:

```python {*}{maxHeight:'370px'}
custom_template = """
{%- for message in messages %}
    {%- if message['role'] == 'system' %}
        {%- set system_message = message['content'] %}
    {%- endif %}
{%- endfor %}
{%- if system_message is defined %}
<|system|>{{ system_message }}<|end|>
{%- endif %}
{%- for message in messages %}
    {%- if message['role'] == 'user' %}
<|user|>{{ message['content'] }}<|end|>
    {%- elif message['role'] == 'assistant' %}
<|assistant|>{{ message['content'] }}<|end|>
    {%- endif %}
{%- endfor %}
{%- if add_generation_prompt %}
<|assistant|>
{%- endif %}
"""
tokenizer.chat_template = custom_template
```

<!--
Templates customizados s√£o para casos muito espec√≠ficos.
Use Jinja2 para criar l√≥gica condicional e loops.
Cuidado: templates errados podem quebrar completamente o modelo!
S√≥ customize se realmente necess√°rio - os templates padr√£o funcionam muito bem.
√ötil para: formatos propriet√°rios, integra√ß√µes especiais, ou experimentos de pesquisa.
-->

---

# Recursos e Documenta√ß√£o

Links √∫teis para aprofundar

Baseado no conte√∫do do [Hugging Face Smol Course - Unit 1.2](https://huggingface.co/learn/smol-course/unit1/2)

**Documenta√ß√£o Oficial:**
- [Chat Templates Basics](https://huggingface.co/docs/transformers/main/en/chat_template_basics)
- [SmolLM3 Model Card](https://huggingface.co/HuggingFaceTB/SmolLM3-3B)
- [TRL Documentation](https://huggingface.co/docs/trl)

<!--
Deixo aqui recursos excelentes para voc√™s continuarem estudando.
A documenta√ß√£o do Hugging Face √© muito completa e bem escrita.
A comunidade √© super ativa - n√£o tenham medo de fazer perguntas!
-->

---
layout: center
class: text-center
---

# Obrigado! üéâ

<div class="pt-12">
  <span class="text-sm opacity-75">
    Baseado no conte√∫do: Hugging Face Smol Course - Unit 1.2
  </span>
</div>

<!--
E √© isso! Espero que tenham gostado e aprendido bastante.
Chat templates podem parecer simples, mas s√£o fundamentais para trabalhar bem com LLMs.
Fiquem √† vontade para perguntar qualquer coisa!
-->
